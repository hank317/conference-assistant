# Conference Assistant ![Static Badge](https://img.shields.io/badge/Apache-2.0-green) ![Static Badge](https://img.shields.io/badge/NewBie-NLP-blue)  
![project_logo](./images/conference_assistant_logo.png)  
[English](README.md) | [ä¸­æ–‡](./README_zh.md)
åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¼šè®®åŠ©æ‰‹ï¼Œé€‚ç”¨äºå…¬å¸å¹´ä¼šã€å­¦æœ¯ç ”è®¨ä¼šç­‰å¤šç§ç±»å‹çš„ä¼šè®®ã€‚è¯¥åŠ©æ‰‹ä½¿ç”¨éå¸¸ç®€ä¾¿ï¼Œä»…éœ€æä¾›ä¼šè®®ç›¸å…³æ–‡æ¡£å³å¯å¯åŠ¨æœåŠ¡ã€‚  
åŒ…æ‹¬ä½†ä¸é™äºä»¥ä¸‹ç”¨é€”ï¼š
- ä¼šè®®åŸºæœ¬ä¿¡æ¯
- å…·ä½“è®®ç¨‹å®‰æ’
- é…’åº—ä¸è¡Œç¨‹é¢„è®¢
- å…¶ä»–æ³¨æ„äº‹é¡¹
## ğŸ¤– åˆ›å»ºåŠ©æ‰‹  
You need to specify the name of the company, school, or organization, and you can also add some instructions to restrict or optimize the assistant's responses.   
```python 
from assistant import LLMAssistant

# Assistant Settings.
firm = 'ä¸­ä¿¡å›½å®‰å®ä¸šé›†å›¢æœ‰é™å…¬å¸' 
instruction = '' 
model = 'qwen-max'

# Conference Information.
file_path = ['./data/agenda_example.xlsx'] 
        
# Create Conference Assistant.
my_assistant = LLMAssistant(firm=firm, instruction=instrction, file_path=file_path, model=model)
```  
You need to specify the large language model used by the assistant. We follow the OpenAI SDK, and currently support the following models: qwenã€kimiã€spark. If there are many documents, it is recommended to use a model that supports longer context input.  
```python
# Add your api_key to the environment variables.
import os
os.environ['API_KEY'] = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'

# Support the following models:
# qwen: qwen-plus, qwen-max  
# kimi: moonshot-v1-8k, moonshot-v1-32k  
# spark: generalv3.5  
``` 
You need to provide additional conference documents, and the assistant will learn and answer based on the content of these documents. We currently support a variety of file formats, including *.doc*, *.docx*, *.xls*, *.xlsx*, *.txt*, *.csv*, *.tsv*, and more. However, we recommend using <mark>*.xlsx*</mark> or <mark>*.csv*</mark> formats for optimal results.   
## ğŸ’« å¿«é€Ÿå¼€å§‹  
Quickly initiate chat, support streaming and non streaming, default streaming.
```python 
# Start a Chat.
query = 'ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ'
stream = False

answer = ''
for chunk in my_assistant.single_chat(query=query, stream=stream):
    answer += chunk
print('answer:', answer)
```
## ğŸ’¥ å¯¹è¯å¢å¼º  
By combining historical session information, the assistant can fully understand each query and answer more accurately.  
When chatting, you need to specify a session ID to manage session information and specify the session rounds that the assistant should consider when answering.
```python 
# Start a Chat.
query = 'ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ'
stream = True
rounds = 3

answer = ''
for chunk in my_assistant.chat(query=query, stream=stream, rounds=rounds):
    answer += chunk
print('answer:', answer)
```
Multiple rounds of session require middleware to store and manage session information. Please change your Redis configuration in the <mark>*config.py*</mark> as shown below:      
```python  
redis_config = {
    'host':'127.0.0.1',
    'port':'7001',
    'password':'xxxxxxxxx'
}
```
## ğŸ¤ ä¸ªæ€§åŒ–å®šåˆ¶
You can also explore additional customized developments triggered through natural language. If you have any needs in this area, please feel welcome to contact us:  
- ğŸ“¬ : zhenhu317@gmail.com  
## ğŸ· è®¸å¯
Conference Assistant is licensed under the [Apache-2.0 License](./LICENSE). 